<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">
  
<head>

  
  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="Attention ◀️ Neural Machine Translation by Jointly Learning to Align and Translate">
  <meta property="og:description" content="2015년 ICLR에서 등장한 논문으로써 처음 번역 분야에 Attention 개념을 사용한 논문으로 알려져있다. 딥린이의 필수 관문이라고 생각하기에 리뷰를 시작하려 한다.">
  <meta property="og:image" content="https://github.com/user-attachments/assets/b3755e25-6654-46bb-9f73-c50b9f0875c6">

  <title>Attention ◀️ Neural Machine Translation by Jointly Learning to Align and Translate</title>
  <meta name="description" content="2015년 ICLR에서 등장한 논문으로써 처음 번역 분야에 Attention 개념을 사용한 논문으로 알려져있다. 딥린이의 필수 관문이라고 생각하기에 리뷰를 시작하려 한다.">

  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.10.0/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>


<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

  <a class="navbar-brand" href="/"><h5><b>YooniversAI</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-1x fa-bars text-themed"></i>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link " href="/projects/">Projects</a>

      <a class="nav-item nav-link active" href="/blog/">Blog</a>

      <a class="nav-item nav-link " href="/about/">About</a>

      

      <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
    </div>
  </div>

</nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1><b>Attention ◀️ Neural Machine Translation by Jointly Learning to Align and Translate</b></h1>

<p class="post-metadata text-muted">
  10 January 2023 -  
  <b>3 mins read time</b>

  <br>Tags: 
    
    <a class="text-decoration-none no-underline" href="/blog/tags#attention">
      <span class="tag badge badge-pill text-primary border border-primary">Attention</span>
    </a>
    
    <a class="text-decoration-none no-underline" href="/blog/tags#review">
      <span class="tag badge badge-pill text-primary border border-primary">review</span>
    </a>
    </p>
<article>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>  
  
<div class="list-group my-3"><a class="list-group-item active disabled text-white">Table of Contents</a><a class="list-group-item list-group-item-action" href="#1-main-contributions">1. Main contributions</a><a class="list-group-item list-group-item-action" href="#2-abstract-introduction">
2. Abstract, Introduction</a><a class="list-group-item list-group-item-action" href="#3-learning-to-align-and-translate">
3. Learning to align and translate</a><a class="list-group-item list-group-item-action" href="#4-experiment">
4. Experiment</a>

</div>

<p><a href="https://arxiv.org/pdf/1409.0473.pdf">paper : Neural Machine Translation by Jointly Learning to Align and Translate</a></p>

<blockquote>
  <p>이 리뷰는 <a href="https://www.youtube.com/watch?v=upskBSbA9cA&amp;feature=youtu.be&amp;ab_channel=JiyangKang">pr12 리뷰</a>를 참고하였습니다.</p>
</blockquote>

<p><br /></p>

<h1 id="1-main-contributions">1. Main contributions</h1>

<p>논문에서의 메인 아이디어는 다음 두 가지이다.</p>

<p><strong>인코더</strong> : <code class="language-plaintext highlighter-rouge">Bidiretional RNN</code>을 적용했다.</p>

<ul>
  <li>논문에서는 \(\overrightarrow{h_j^\top}\)와 \(\overleftarrow{h_j^\top}\)를 concatenate하여 인코더의 hidden state로 사용한다.</li>
</ul>

<p><strong>디코더</strong> : <code class="language-plaintext highlighter-rouge">Proposed an extension model(attention network)</code></p>

<ul>
  <li>attention mechanism : Hidden states들의 weighted sum을 계산</li>
</ul>

<p><br /></p>

<h1 id="2-abstract-introduction">2. Abstract, Introduction</h1>

<p>NMT(Neural Machine Translation)은 machine translation 분야의 최근에 제안된 방식이다. 특히, 최근의 모델들은 encoder-decoder 구조에 속한다. Input 문장을 encoder network을 통해 fixed size vector로 인코딩하고, 이 vector를 다시 디코딩하는 과정으로 번역을 진행한다.</p>

<p>인코딩 네트워크와 디코딩 네트워크는 동시에 훈련되는데 여기서 “<strong><u>고정된 길이의 벡터(fixed length vector)</u></strong>“로 압축해야하는 것은 잠재적 문제이다. 이는 학습된 corpus 문장보다 긴 문장을 처리하기 힘듦을 의미한다. 실제로 조경현 교수님의 <a href="https://arxiv.org/pdf/1406.1078.pdf">논문</a>에서 이러한 성능 저하를 보여주고있다.</p>

<p><img src="https://user-images.githubusercontent.com/120036648/211738334-847794bc-04b7-41d8-89a9-460714a4ab1e.PNG" alt="bleuscore" width="80%" class="center" /></p>

<ul>
  <li>실제로 문장의 길이가 길어질수록 BLEU 스코어가 급감함을 볼 수 있다. 학습에 사용된 코퍼스의 수 역시 짧을 수록 더 성능이 안좋음을 볼 수 있다.</li>
</ul>

<p>논문에서는 이러한 단점을 극복하기 위해 제안된 방법으로 soft-alignments(이하 attention) 작업을 진행한다.</p>

<p><strong>Soft-alignment와 Hard-alignment의 차이</strong></p>

<ul>
  <li>hard-alignment(attention)는 시계열의 순서 그대로 1:1로 매핑하여 해석하는 방식. 어순이 다른 두 언어를 인식하지 못한다.</li>
  <li>soft-alignment(attention)는 정보를 ‘모두’ 확인하며 어순이 달라도 학습을 통해 인식함을 의미한다. <strong>이하 attention으로 표현한다.</strong></li>
</ul>

<p><br /></p>

<h1 id="3-learning-to-align-and-translate">3. Learning to align and translate</h1>

<p>앞서 간략히 언급했듯이 인코더에는 Bidirectional RNN을 사용함으로써 앞으로 나올 문장 및 단어는 물론이고, 이전에 나왔던 단어까지 모델에 반영한다. 디코딩에는 attention layer를 통해 번역될 소스 문장을 찾는다.</p>

<p>네트워크에 대한 설명은 다음 수식들로 간략하게 설명하고 넘어가겠다.</p>

<p><img src="https://user-images.githubusercontent.com/120036648/211738438-b4cffacc-0248-4ac5-96d2-247001779c5e.PNG" alt="attention" width="60%" class="center" /></p>

<p>Bidirectional encoder에서 hidden states 벡터 \(h\)는 순방향 벡터 \(\overrightarrow{h_j^\top}\)와 역방향 벡터 \(\overleftarrow{h_j^\top}\)를 concatenate하여 구한다.</p>

<p>다음으로, <em>alignment model</em> \(e_{ij}\)는</p>

\[e_{ij} = a(s_{i-1}, h_j) = v_a^\top\text{tanh}(W_as_{i-1} + U_ah_j)\]

<p>이고, 이때 \(W_a\)와 \(U_a\) 그리고 \(v_a\)는 weight matrices이다.</p>

<p>이렇게 구한 \(e_{ij}\)의 각각의 확률값으로부터 그 비율을 계산하여 weight 값인 \(\alpha\)를 구한다.</p>

\[\alpha_{ij} = \frac{exp(e_{ij})}{\sum^{T_x}_{k=1}exp(e_{ik})}\]

<p>이 weight들과 hidden state의 weighted sum인 <em>Context vector</em> \(c_i\)를 계산한다.</p>

\[c_i = \sum^{T_x}_{j=1}\alpha_{ij}h_j\]

<p>이 i번째 Context vector는 i-1번째의 \(s_{i-1}\)와 \(y_{i-1}\)과 함께 계산되어, 다음 i번째 \(s_i\)를 구하는데 사용된다.</p>

\[s_i = f(s_{i-1}, y_{i-1}, c_i)\]

<p>여기서 중요한 점은 context vector의 인덱스에서 확인할 수 있듯이, 매 단어마다 context vector를 구해준다는 점이다.  seq2seq 모델은 context vector 값이 상수로 계산된다. 즉, 인코딩이 끝나면 딱 한 번 계산되고, 그 vector로부터 디코딩을 실행하지만, attention은 매 단어마다 다시 계산된다.</p>

<p>이러한 방식으로 계산되는 context vector는 fixed-length로부터 자유롭게 해준다. 가변길이의 문장을 인코딩하여 고정길이의 벡터로 만들고, 다시 디코딩하여 가변 길이 벡터로 만드는 것은 모델의 성능을 저하시킬 수 있다고 논문에서는 주장한다.</p>

<p><br /></p>

<h1 id="4-experiment">4. Experiment</h1>

<p>논문에서는 영어와 불어 번역으로 테스트를 했고, 약 348M개의 데이터를 사용하여 테스트를 진행했다.</p>

<p><img src="https://user-images.githubusercontent.com/120036648/211738334-847794bc-04b7-41d8-89a9-460714a4ab1e.PNG" alt="bleuscore" width="80%" class="center" /></p>

<p>위 실험 결과에서 RNNenc는 이전 모델이고, RNNsearch는 논문에서 제안한 모델이다. 새로 제안한 모델이 이전 모델과 비교했을 때 더 좋은 성능을 내는 것을 볼 수 있다. 특히, 50개 이상의 단어로 이루어진 긴 문장으로 학습했을 경우에 문장의 길이가 길어져도 성능 하락을 잘 방어하는 모습이다.</p>

<p><img src="https://user-images.githubusercontent.com/120036648/211738634-3ba509ae-3fd3-4f34-aa0c-277c5e42b291.PNG" alt="attention_result" width="80%" class="center" /></p>

<p>*가 붙은 RNNsearch-50은 긴 시간 동안 학습한 경우인데, SMT(Statistical machine translation)에서 유명한 모델인 Moses보다 좋은 성능을 발휘하는 모습을 확인할 수 있다. (여기서 No UNK\(^\circ\)는 모르는 단어를 제외한 경우를 의미한다.)</p>

<p><strong>추가적인 실험 및 결과는 논문에 자세히 나와있으니 참고하자.</strong></p>

</article>


</div>
  </main>
  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Doyeon Yoon</strong>
  </small>

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="mailto:ydy89899@gmail.com"
       style="color: #6c757d"
       onMouseOver="this.style.color='#db4437'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fas fa-envelope fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.github.com/ydy8989"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/doyeon-yoon"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-1x"></i>
    </a>

</div><small id="attribution">
    theme <a href="https://github.com/YoussefRaafatNasry/portfolYOU">portfolYOU</a>
  </small>

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>
<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="doyeon" data-description="Support me on Buy me a coffee!" data-message="Thank you for visiting." data-color="#007bff" data-position="Right" data-x_margin="16" data-y_margin="16"></script></body>

</html>
