<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">
  
<head>

  
  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="Transformer ◀️ Attention is all you need">
  <meta property="og:description" content="이번 포스팅은 최근 2년간의 NLP task에서 SOTA를 기록했던 모델들의 모태가 되는 Transformer(NIPS 2017)에 대한 논문을 리뷰하려 한다.">
  <meta property="og:image" content="https://user-images.githubusercontent.com/120036648/206411545-4e7eea0d-06e9-43e8-a6b7-638a03855365.png">

  <title>Transformer ◀️ Attention is all you need</title>
  <meta name="description" content="이번 포스팅은 최근 2년간의 NLP task에서 SOTA를 기록했던 모델들의 모태가 되는 Transformer(NIPS 2017)에 대한 논문을 리뷰하려 한다.">

  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.10.0/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>


<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

  <a class="navbar-brand" href="/"><h5><b>YooniverAI</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-1x fa-bars text-themed"></i>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link " href="/projects/">Projects</a>

      <a class="nav-item nav-link active" href="/blog/">Blog</a>

      <a class="nav-item nav-link " href="/about/">About</a>

      

      <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
    </div>
  </div>

</nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1><b>Transformer ◀️ Attention is all you need</b></h1>

<p class="post-metadata text-muted">
  08 December 2022 -  
  <b>8 mins read time</b>

  <br>Tags: 
    
    <a class="text-decoration-none no-underline" href="/blog/tags#transformer">
      <span class="tag badge badge-pill text-primary border border-primary">transformer</span>
    </a>
    
    <a class="text-decoration-none no-underline" href="/blog/tags#review">
      <span class="tag badge badge-pill text-primary border border-primary">review</span>
    </a>
    </p>
<article>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>  
  
<p>아무튼 제목 때문에 NLP에 대한 지식이 없었던 나로서는 이 논문이 attention 구조가 등장한 첫 논문이라고 생각했었다. (attention 구조의 첫 등장은 <a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a>라는 논문이다.)</p>

<blockquote>
  <p>이전 <a href="https://ydy8989.github.io/2020-11-15-attention/">포스팅</a>에서 리뷰하였으니 attention에 대한 정보가 없다면, 간단히 참고하는 것을 추천한다.</p>
</blockquote>

<p><br /></p>

<h1 id="1-motivation">1. Motivation</h1>

<p>Transformer는 바로전 리뷰한 논문인 <a href="https://ydy8989.github.io/2020-11-15-attention/">Neural Machine Translation by Jointly Learning to Align and Translate</a>에서의 단점을 보완한 architecture이다. 해당 논문에서는 alignment(이하 attention)을 통해 기존 seq2seq 구조에서 <u>'고정된 길이로 압축되는 context vector'</u>의 단점을 극복했다고 언급했다. 하지만, 여전히 BiLSTM을 이용하며, 이 같은 sequential한 구조는 학습 속도와 긴 문장의 학습에 한계를 가지고 있었다.</p>

<p>논문에서는 attention mechanism에 기반한 transformer라는 모델을 제안하고, 기계 번역 task에서 그간 보여주지 못했던 최고 수준의 결과를 보여주었다.</p>

<p><br /></p>

<h1 id="2-model-architecture">2. Model Architecture</h1>

<p><strong>Transformer</strong>는 encoder와 decoder로 이루어져있다. Symbol representations \((x_1, \dots, x_n)\)를 encoder의 input으로 받으면 continuous한 sequence \(\mathbb{z}=(z_1, \dots, z_n)\)로 mapping해주고, 이 \(\mathbb{z}\)는 다시 decoder를 거쳐 \((y_1, \dots, y_m)\)가 생성된다.</p>

<p>각 단계는 auto-regressive(자동회귀)하며, 다음 단계의 symbol 생성시 이전단계의 symbol을 추가 입력으로 받는다.</p>

<p><img src="https://user-images.githubusercontent.com/120036648/206434262-eede90f6-3307-4eee-9bfa-30dc28c92168.png" alt="image" width="50%" class="center" /></p>

<p><br /></p>

<h2 id="21-encoder-and-decoder-stacks">2.1. Encoder and Decoder Stacks</h2>

<ul>
  <li>
    <p><strong>Encoder</strong> : 위 그림과 같이 인코더는 N=6개의 동일한 레이어 스택으로 구성된다. 각 스택은 두 개의 하위 레이어로 구성되는데,</p>

    <ol>
      <li>multi-head self-attention</li>
      <li>positionwise fully connected feed-forward network</li>
    </ol>

    <p>의 두 레이어로 구성되어 있다.</p>
  </li>
  <li>
    <p><strong>Decoder</strong> : encoder와 마찬가지로 N=6 개의 동일한 레이어 스택으로 구성되어있다. 단, encoder와 달리 추가적인 layer가 포함되어 있다.</p>

    <ol>
      <li>masked multi-head attention</li>
    </ol>

    <p>이는 디코더가 출력을 생성할 시, 다음 출력에서 정보를 얻는 것을 막기 위해 masking을 사용하는 layer이다.</p>
  </li>
</ul>

<p>모든 layer는 공통적으로 “Add &amp; Norm” layer를 뒤에 붙이고 있는데, 이는 residual connection으로 구성되어 있다. 각 layer의 input은 x이고 output은 sublayer(x)라고 할 때, 각 layer의 output은 LayerNorm(x + sublayer(x))로 이루어진다. 기존 정보 x를 입력 받으면서, 추가적인 residual 부분만을 받기 때문에 학습 속도가 높고, 초기 모델 수렴 속도가 빠르다.</p>

<p>기본적인 encoder-decoder 구조라는 것을 알았으니 다음 절부터는 좀 더 자세히 인코더와 디코더를 구성하는 layer에 대해 알아보고, 마지막에 작동 원리를 간단히 요약하며 마무리할 예정이다.</p>

<p><br /></p>

<h2 id="22-attentionself-attention">2.2. Attention(self-attention)</h2>

<p>먼저, transformer에서 가장 핵심이 되는 multi-head attention layer에 대해 알아보도록 하자.</p>

<p>논문에서 어텐션 함수는 <code class="language-plaintext highlighter-rouge">Query</code>와 <code class="language-plaintext highlighter-rouge">Key</code>-<code class="language-plaintext highlighter-rouge">value</code> pair의 관계를 output으로 mapping(함수)한다고 말하고 있다. 즉, Input 값으로 받은 Query와 Key-Value vector를 바탕으로 output을 도출하는 함수이며, 이때 output은 value값의 <code class="language-plaintext highlighter-rouge">weighted sum</code>으로 계산된다고 한다.</p>

<p>트랜스포머 구조에서의 attention은 <code class="language-plaintext highlighter-rouge">multi-head attention</code>으로 이뤄져있고, 이 multi-head attention은 <code class="language-plaintext highlighter-rouge">scaled dot product attention</code>으로 구성되어 있다. <del>무슨소리냐고? 이제부터 읽어보자</del></p>

<p><br /></p>

<h3 id="221-scaled-dot-product-attention">2.2.1. Scaled Dot-Product Attention</h3>

<p><img src="https://user-images.githubusercontent.com/120036648/206434379-89a1e838-8144-4d64-b899-f9e08cceed80.png" alt="image" width="50%" class="center" /></p>

<p>Scaled Dot-Product attention layer는 위와 같다. <br />
모든 K(key)로 Q(query)를 내적하고 이를 \(\sqrt d_k\)로 나눠 스케일링한다. <br />
Masking은 패딩된 부분에 대해서는 attention 연산을 하지 않는 과정이며 이는 그림과 같이 옵션이다. <br />
이후 각 벡터를 softmax를 통해 vector에서 token이 갖는 값의 비중을 구하고, 이를 마지막으로 V(value)와 다시 곱하여 attention score를 구한다.</p>

<p>식은 다음과 같다.</p>

\[Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt d_k})V\]

<p>논문에서는 attention fucntion은 보통 다음 두 가지를 사용한다고 말한다.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">additive attention</code> : 단일 hidden layer의 feed-forward 네트워크를 사용하여 compatibility function를 계산하고, \(d_k\)가 작을 수록 성능이 좋다고 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">dot-product attention</code> : 반대로 \(d_k\)가 클수록 더 빠르고 효율적인 연산이 가능하다고 한다.</li>
</ul>

<p>한편, 그림에서 보듯 optional하게 masking layer를 사용해야 하는 경우가 있다. 이는 아래 multi-head attention의 마지막 부분에서 설명하도록 한다.</p>

<p><br /></p>

<h3 id="222-multi-head-attention">2.2.2. Multi-Head Attention</h3>

<p>Multi-Head Attention은 앞서 설명했던 attention 연산을 여러번 수행하는 거라 생각하면 된다.</p>

<p><img src="https://user-images.githubusercontent.com/120036648/206434452-1b884858-308b-4c05-b1ed-ce8af4e3a2fb.png" alt="image" width="50%" class="center" /></p>

<p>위의 그림을 살펴보면 앞선 절에서 설명했던 Scaled Dot-Product Attention 연산이 \(h\)번 합쳐진 모습을 볼 수있다.</p>

<p>논문에서는 \(d_{model}\) 차원의 <code class="language-plaintext highlighter-rouge">key</code>, <code class="language-plaintext highlighter-rouge">value</code>, <code class="language-plaintext highlighter-rouge">query</code>들을 사용하는 것보다, 서로 다르게 학습된 linear projection을 사용하여 \(d_k\), \(d_v\), \(d_q\) 각각의 차원으로 h 번 학습시키는 것이 낫다고 한다. 이 때, 단순히 \(h\)번의 횟수는 sublayer의 같은 부분이 \(h\)개 존재함을 의미한다. 이렇게 통과한 \(h\)쌍의 \(d_v\)차원의 출력은 Concatenate되어 출력된다.</p>

<p>다시 돌아와서 단어(token)부터 layer를 통과하기까지를 그림으로 살펴보자.</p>

<p><img src="https://user-images.githubusercontent.com/120036648/206434489-b17c7726-7eb7-43d7-a084-668f84230028.png" alt="image" width="80%" class="center" /></p>

<blockquote>
  <p>image reference : <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></p>
</blockquote>

<p>위 그림처럼 우선 토크나이징 된 “Thinking”이라는 단어를 \(X_1\)라는 vertor로 임베딩을 했다고 가정하자(실제 transformer의 input은 토큰들의 embedding vector가 합쳐진 형태의 embedding matrix로 들어가게 된다). 임베딩 벡터 \(X_1\)에 \(W^Q\), \(W^K\), \(W^V\)라는 가중치 행렬을 곱해 \(q_1\), \(k_1\), \(v_1\) 를 얻는다. <br />
가중치 행렬의 차원은 \(W_i^{Q, K, V}=\mathbb{R}^{d_{model}\times d_k}\)로 만든다. 이 때, \(d_k\)는 \(d_{model}\)을 \(h\)로 나눈 값이다. 그 이유는 multi-head attention의 마지막에 \(h\)개의 attention layer를 concatenate하기 때문이다.</p>

<blockquote>
  <p>논문에서는 \(h=8\)의 attention layer를 사용하였고, 임베딩 차원은 512 차원으로 사용했다고한다.</p>
</blockquote>

<p>이렇게 완성된 각각의 \(q_1\), \(k_1\), \(v_1\)는 Dot-Product attention 연산을 통해 \(z_1\)로 출력된다. 하지만 앞서 말했듯이, Transformer의 input은 문장의 embedding vector를 모두 합친 embedding matrix 형태로 들어가게된다. 따라서 attention연산은 아래와 같이 \(q_1, \dots, q_n\)을 합친 \(Q\), \(k_1, \dots, k_n\)을 합친 \(K\), \(v_1, \dots, v_n\)을 합친 \(V\)의 형태를 거쳐 \(Z\)로 출력된다.</p>

<p><img src="https://user-images.githubusercontent.com/120036648/206434522-ae0b4e06-55b7-49ab-9a7f-7f333ccd01d6.png" alt="image" width="80%" class="center" /></p>

<blockquote>
  <p>image reference : <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></p>
</blockquote>

<p>이렇게 완성된 하나의 \(Z\)는 multi-head attention의 \(h\)개 중 하나의  output이며, 이러한 attention 연상이 총 8번(논문 기준) 수행된다. 여기서 각 attention head에서 만들어진 가중치 행렬  \(W^Q\), \(W^K\), \(W^V\)은 중복되지 않고, 각 head마다 새롭게 계산된다.</p>

<p>이렇게 head마다 계산된 뒤 concatenate된 \(Z_0, \dots, Z_7\)은 마지막 추가 웨이트를 통해 \(Z\)로 출력된다.</p>

<p>논문에서는 총 세 가지 종류의 Attention Layer를 사용한다.</p>

<ol>
  <li>Encoder Self-Attention</li>
  <li>Masked Decoder Self-Attention</li>
  <li>Encoder-Decoder Attention</li>
</ol>

<p><br /></p>

<h4 id="encoder-self-attention">Encoder Self-Attention</h4>

<p>앞서 설명한 내용을 기본으로 생각하면 이해하기 쉽다. 문장의 단어들이 앞 뒤의 단어들과 self-attention 연산을 수행하면서 attention score를 계산하는 과정이다.</p>

<p><br /></p>

<h4 id="masked-decoder-self-attention">Masked Decoder Self-Attention</h4>

<p>Decoder의 첫번째 input이 들어온 뒤의 multi-head attention layer로써, 출력 단어가 자기보다 앞서 이미 앞에 나왔던 단어들만 참고해서 연산하는 attention layer이다. 뒤쪽에 나온 단어까지 참고해 앞을 예측하게 한다면 이는 일종의 cheating처럼 작용해 auto-regressive를 수행하지 못하는 모델이 된다.</p>

<p>이를 위해서는 현재 진행중인 단어보다 뒤쪽 단어들에 대한 masking 작업이 필요하다. Masking의 방식은 아래와 같다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">query/key</th>
      <th style="text-align: center">I</th>
      <th style="text-align: center">am</th>
      <th style="text-align: center">a</th>
      <th style="text-align: center">boy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">I</td>
      <td style="text-align: center">23</td>
      <td style="text-align: center">\(-\infty\)</td>
      <td style="text-align: center">\(-\infty\)</td>
      <td style="text-align: center">\(-\infty\)</td>
    </tr>
    <tr>
      <td style="text-align: center">am</td>
      <td style="text-align: center">15</td>
      <td style="text-align: center">27</td>
      <td style="text-align: center">\(-\infty\)</td>
      <td style="text-align: center">\(-\infty\)</td>
    </tr>
    <tr>
      <td style="text-align: center">a</td>
      <td style="text-align: center">14</td>
      <td style="text-align: center">20</td>
      <td style="text-align: center">23</td>
      <td style="text-align: center">\(-\infty\)</td>
    </tr>
    <tr>
      <td style="text-align: center">boy</td>
      <td style="text-align: center">11</td>
      <td style="text-align: center">18</td>
      <td style="text-align: center">22</td>
      <td style="text-align: center">25</td>
    </tr>
  </tbody>
</table>

<p>\(Q\times K^T\)를 통해 만들어진 행렬을 위와 같이  참고하지 않을 부분을 \(-\infty\)로 할당해 줌으로써 softmax의 output이 0으로 수렴하게 만든다.</p>

<p><br /></p>

<h4 id="encoder-decoder-attention">Encoder-Decoder Attention</h4>

<p>Decoder 파트의 두 번째 multi-head attention layer로써, Query는 decoder에 있고, 각각의 Key와 value는 Encoder에 있는 attention 파트이다. “난 널 좋아해”라는 문장을 “I like you”로 번역할 때, “like”라는 단어가 “난”, “널”, “좋아해” 중에서 어떤 단어에 더 많은 가중치를 두는지를 참조하는 과정이다.</p>

<p><br /></p>

<h4 id="self-attention">Self-Attention</h4>

<p>모든 Encoder와 Decoder에서 사용된다. 학습하고자 하는 문장에서 각 단어에 대해 해당 단어가 포함된 자기 자신의 문장의 어떤 단어와 얼만큼 연관있는지를 계산하는 방식을 의미한다.</p>

<blockquote>
  <p>정형데이터의 상관관계 heatmap을 생각하면 이해하기 쉽다.</p>
</blockquote>

<p><br /></p>

<h2 id="23-positional-encoding">2.3. positional encoding</h2>

<p>인코딩 및 디코더의 입력값마다 상대적인 위치정보를 더해주는 방법이다. Transformer는 RNN이나 CNN을 사용하지 않는만큼 input word의 순서를 알려주는 layer를 포함하지 않는다. 따라서 input 단어들의 순서를 알려주는 구조가 필요하다.</p>

<p>Positional encoding은 다음과 같은 주기 함수를 사용하여 각 단어의 상대적 위치정보를 학습할 수 있게 도와준다.</p>

\[PE_{(pos, 2i)} = sin(pos /10000^{2i/d_{model}})\\
PE_{(pos, 2i+1)} = cos(pos /10000^{2i/d_{model}})\]

<p>여기서 \(pos\)는 각각의 단어 번호(문장 내 단어의 순서)를 의미하고, \(i\)는 각 단어의 임베딩 값의 위치를 의미한다. 이렇게 구한 PE는 input으로 들어온 임베딩 행렬과 element-wise로 더해져서 사용된다.</p>

<ul>
  <li>예를 들어 “I am a boy”라는 문장을 512차원의 vector로 임베딩한다고 가정하자. 각 단어는 512차원의 길이를 가지는 vector가 되고, 전체 문장은 4 X 512의 matrix로 임베딩 되어 input으로 들어갈 것이다. 이때, Positional Encoding(PE)역시 4 X 512 크기의 matrix로 구해지게 된다.</li>
  <li>워드 임베딩 행렬과 PE 행렬은 각각의 자리를 더해 최종적으로 4 X 512 임베딩으로 transformer에 사용된다.</li>
</ul>

<p>Positional encoding에 사용되는 함수는 꼭 주기함수가 될 필요는 없고, 단지 상대적 위치 정보가 기록될 수 있는 함수면 상관없다. 실제로 이후 발표되는 모델들에 sin, cos함수가 사용되지 않는 경우도 많다.</p>

<p>논문에서는 주기함수를 사용함으로써 학습 데이터보다 더 긴 문장이 들어오더라도 인코딩 값을 할당할 수 있다는 점을 장점으로 말하고 있다.</p>

<p><br /></p>

<h2 id="24-position-wise-feed-forward-networks">2.4. Position-wise Feed-Forward Networks</h2>

<p>encoder와 decoder 각 파트의 마지막 attention sub-layers에는 FC feed forward network가 포함되어있다. 이는 각 위치마다 개별적으로 동작한다. 여기서는 ReLU가 포함된 두 개의 선형 변환으로 구성된다.</p>

\[\text{FFN(x)} = max(0, xW_1 + b_1)W_2 + b_2\]

<p>이 선형 변환은 다른 위치에서 동일하지만, 각 레이어에서 다른 파라미터를 사용한다. 앞서 언급했듯 논문에서는 \(d_{model}=512\)로 사용했고, \(d_{ff}=2048\)를 사용했다.</p>

<p><br /></p>

<h1 id="3-why-self-attention">3. Why Self-Attention?</h1>

<p>논문에서는 \((x_1, \dots, x_n)\rightarrow(z_1,\dots,z_n)\)의 mapping에 self-attention이 적합한 이유를 기존의 recurrence 모델 및 convolutional 모델과 비교하여 말하고 있다. 간단하게 요약하자면 다음 표로 나타낼 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/120036648/206434655-8629954a-ab6e-4b11-bbe6-063e5e598e91.png" alt="image" width="80%" class="center" /></p>

<p>논문에서는 \(n\)이 짧을 때는 비슷하기 때문에 가장 긴 길이일 때를 비교하였다. 최대 길이가 길 때 long-term dependency가 self-attention에서 매우 낮은 것을 확인할 수 있다. Layer당 계산량이 현저히 작으며, recurrent 모델과는 다르게 병렬화가 가능하다. 기존 RNN 모델의 경우에는 모델이 word의 순서에 맞게 계산해야 했기 때문에 계산 병렬화가 불가능했지만, transformer는 positional encoding으로 이를 극복하였다.</p>

<p><br /></p>

<h1 id="4-experiments">4. Experiments</h1>

<p>Transformer 모델은 논문에서 볼 수있듯이 기본 base model로도 엄청난 성능을 보여주고 있다. 특히, Big model의 경우 SOTA를 경신하는 모습을 보여줬다. 더 자세한 실험 결과 및 성능은 직접 논문을 참고하는 것을 추천한다.</p>


</article>


</div>
  </main>
  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Doyeon Yoon</strong>
  </small>

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="mailto:ydy89899@gmail.com"
       style="color: #6c757d"
       onMouseOver="this.style.color='#db4437'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fas fa-envelope fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.github.com/ydy8989"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/doyeon-yoon"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-1x"></i>
    </a>

</div><small id="attribution">
    theme <a href="https://github.com/YoussefRaafatNasry/portfolYOU">portfolYOU</a>
  </small>

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>
<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="doyeon" data-description="Support me on Buy me a coffee!" data-message="Thank you for visiting." data-color="#007bff" data-position="Right" data-x_margin="16" data-y_margin="16"></script></body>

</html>
